# ResNet / Residual Learning Cheat Sheet

## 0) Residual Learning (핵심 아이디어)

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 (왜 필요한가 / 역할) |
| --- | --- | --- | --- |
| Residual Addition | Eqn.(1): y = F(x, {Wi}) + x | `out = out + identity` | **비유**: “새 글을 처음부터 쓰는 게 아니라, 원본 초안을 그대로 두고 빨간펜으로 수정사항만 적는 방식.”<br><br>**수학적 근거**: + x가 붙는 순간 역전파가 ∂L/∂x = ∂L/∂y · (1 + ∂F/∂x) 형태가 되어, 최소한 그대로 지나가는 경로가 항상 존재합니다. 즉, 깊어져도 기울기가 “막다른 골목”으로만 흐르지 않습니다.<br><br>**핵심**: 기존 모델은 정답 y를 처음부터 만들어야 했지만, 잔차학습은 이미 알고 있는 입력 x를 기준으로 필요한 변화분만 학습하도록 문제를 단순화합니다. 이 덕분에 깊이가 늘어나도 “아무것도 안 하기(F=0)”라는 쉬운 해가 항상 가능해지고, 깊어질수록 학습이 어려워지는 degradation 문제가 구조적으로 완화됩니다. |
| Residual Function | Figure 2 | `conv → bn → relu → conv → bn` | **비유:** “집을 새로 짓는 게 아니라, 이미 튼튼한 집(입력 x)에 생긴 흠집만 메우는 보수공사.”<br><br>**수학적 근거:** 잔차는 사실상 F(x) ≈ y - x를 학습하는 셈이라, 목표가 전체 y가 아니라 차이(변화분)로 바뀝니다. 특히 최적해가 ‘거의 그대로’라면 정답은 F(x)=0 근처로 몰려 학습이 쉬워집니다.<br><br>**핵심:** 기존 모델은 매 층마다 입력을 새롭게 변환해 정답에 가까워져야 했기 때문에, 깊어질수록 불필요한 변화가 누적되기 쉬웠습니다. 잔차 함수는 입력 x를 기준으로 정말로 필요한 변화분만 학습하도록 문제를 바꿔, 아무 변화가 필요 없을 경우에는 그대로 두는 것(F(x)=0)이 가장 쉬운 선택이 되게 만듭니다. 이 덕분에 네트워크는 깊어져도 입력을 망가뜨리며 헤매지 않고, 학습이 자연스럽게 “유지 + 필요한 만큼만 수정”하는 방향으로 정렬됩니다. |

---

## 1) Shortcut 연결 방식 (Option A / B)

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| Identity Shortcut | Option A Eqn.(1): y = F(x, {Wi}) + x | `nn.Identity()` | **비유:** “복잡한 골목길(Residual branch) 옆에 직진 고속도로 하나를 공짜로 깔아둔 것.”<br><br>**수학적 근거:** shortcut이 그대로 x라서, 이 경로의 도함수는 항상 I(항등) 입니다. 학습 신호가 ‘변환을 거치지 않고’ 지나갈 수 있습니다.<br><br>**핵심:** 기존의 깊은 네트워크에서는 학습 신호가 여러 변환을 거치며 약해지거나 왜곡되기 쉬웠습니다. Identity shortcut은 입력 x를 아무 변화 없이 그대로 전달하는 경로를 추가해, 모델이 어떤 층에서도 “굳이 변환하지 않아도 된다”는 선택지를 항상 갖게 합니다. 이 경로는 파라미터를 전혀 늘리지 않으면서도, 깊어질수록 학습이 막히는 문제를 구조적으로 완화해 모델의 성능이 아니라 학습 가능성 자체를 안정화하는 역할을 합니다.<br><br>입력과 출력의 차원이 같을 때 사용하는 기본 shortcut (Option A의 기본형) |
| Projection Shortcut | Option B  \<br>Eqn.(2): (y = F(x) + W_s x) | `ShortcutProjection` | **비유**: “플러그 모양이 달라서 안 꽂히면, 어댑터(변환 젠더)로 규격을 맞추는 것.”<br><br>**수학적 근거**: W_s (보통 1×1 conv)가 채널/해상도를 바꿔 덧셈이 가능한 동일 차원으로 보냅니다. ‘add 가능 조건’을 만족시키는 수학적 장치입니다.<br><br>**핵심**: 스테이지가 바뀌는 지점에서는 해상도나 채널 수가 달라져 입력 x를 그대로 더할 수 없는 문제가 생깁니다. Projection shortcut은 이 불일치를 단순히 피하는 대신, 1×1 conv를 통해 어떤 정보를 남기고 어떤 정보를 줄일지를 학습하면서 입력을 다음 스테이지와 호환되는 형태로 변환합니다. 그 결과, 구조적 제약(add 가능 조건)을 만족시키는 동시에 다운샘플링 구간에서도 중요한 정보가 끊기지 않고 전달되도록 합니다.<br><br>차원이 바뀌는 구간에서 성능과 안정성을 우선할 때 선택하는 Option B |
| Zero-padding Shortcut | Option A 응용 (Figure 4 right) | `ShortcutZeroPad` | **비유**: “단체 사진에서 인원이 모자라면, 빈 의자(0)를 옆에 두어 줄을 맞추는 것.”<br><br>**수학적 근거**: 채널을 [x, 0]처럼 0으로 패딩해 차원만 맞춘 뒤 F(x)+x를 수행합니다. W_s를 학습하지 않는 고정 매핑이라 파라미터가 늘지 않습니다.<br><br>**핵심**: CIFAR-10 설정에서는 채널 수가 바뀌는 구간에서도 추가 파라미터 없이 residual 구조를 유지할 필요가 있었습니다. Zero-padding shortcut은 학습 가능한 변환을 아예 도입하지 않고, 입력에 0을 덧붙여 차원만 맞춘 채 그대로 더하는 방식을 선택합니다. 이 설계는 성능을 최대화하기 위한 선택이 아니라, 잔차 연결 자체가 학습을 쉽게 만든다는 효과를 추가 파라미터의 도움 없이도 확인하기 위한 실험적 통제 장치입니다.<br><br>실험적 통제를 위해 projection을 일부러 쓰지 않는 Option A (CIFAR-10 설정) |

---

## 2) Basic Residual Block

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| Basic Block | Figure 5 (left) | `BasicBlockV1` | **비유:** “한 번에 인테리어를 갈아엎지 말고, 사포질→한 번 더 사포질처럼 작은 개선을 두 번 누적하는 방식.”<br><br>**수학적 근거:** F(x)를 3×3 conv 두 번으로 구성해 ‘작은 변화’를 만들고, 마지막에 +x로 합칩니다. 깊게 쌓아도 각 블록은 ‘미세 업데이트’로 동작해 최적화가 안정적입니다.<br><br>**핵심:** 기존의 깊은 네트워크에서는 여러 층이 한꺼번에 큰 변화를 만들려다 학습이 불안정해지기 쉬웠습니다. Basic Residual Block은 한 블록 안에서 3×3 conv 두 개로 아주 작은 변화만 들고, 이를 입력과 더해 누적하도록 설계됩니다. 그 결과 각 블록은 “크게 바꾸는 단계”가 아니라 조금씩 고쳐 나가는 반복 개선 단계로 동작하며, 네트워크가 깊어져도 학습이 무너지지 않는 구조가 됩니다. |
| Post-activation | Figure 2 | `relu(out)` (add 이후) | **비유:** “원본과 수정본을 합친 뒤, 최종 도장(활성화)을 찍어 ‘이 값은 통과/보류’를 결정.”<br><br>**수학적 근거:** v1은 y=ReLU(F(x)+x). 즉 비선형성이 합산 결과 전체에 적용됩니다.<br><br>**핵심:** Residual 구조에서는 학습 신호가 더하기를 통해 잘 흐르더라도, 그 이후에 어떤 연산을 두느냐에 따라 흐름이 다시 막힐 수 있습니다. ResNet v1은 잔차를 더한 뒤에 ReLU를 적용해 표현력을 높이는 선택을 했지만, 이로 인해 경우에 따라 shortcut을 통해 전달된 신호까지 함께 차단될 가능성이 생깁니다. 이 한계를 인식하고, 이후 ResNet v2에서는 비선형성을 더하기 이전으로 옮겨 잔차 경로와 identity 경로의 흐름을 더 부드럽게 유지하도록 구조를 개선했습니다. |

---

## 3) Bottleneck Block (깊은 ResNet)

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| Bottleneck Block | Figure 5 (right) | `BottleneckV1` | **비유:** “큰 짐을 압축(좁은 통로 통과)→이동→재포장해서 운송비를 줄이는 택배.”<br><br>**수학적 근거:** 1×1로 채널을 줄인 뒤(압축), 3×3을 작아진 채널에서만 돌리고, 다시 1×1로 확장합니다. 3×3 연산이 가장 비싸니, 그 구간을 얇게 만드는 게 포인트입니다.<br><br>**핵심**: ResNet을 더 깊게 쌓으려 할수록 문제는 표현력이 아니라 연산량과 최적화 비용이 급격히 커진다는 점이었습니다. Bottleneck 구조는 깊이를 줄이지 않으면서도 가장 비용이 큰 3×3 연산을 채널이 줄어든 상태에서만 수행하도록 설계해, 계산 부담을 구조적으로 억제합니다. 그 결과, “깊이는 유지하되 비싼 연산은 최소화”하는 방식으로 아주 깊은 ResNet을 실제로 학습 가능한 모델로 만듭니다. |
| Channel Expansion | Table 1 | `expansion = 4` | **비유:** “압축해서 옮긴 뒤, 목적지에서 더 큰 서랍(4배 채널)에 정리해 활용도를 높이는 것.”<br><br>**수학적 근거:** `expansion=4`는 bottleneck 출력 채널이 4배로 커진다는 뜻이고, shortcut과 더하려면 차원을 맞추기 위해 종종 projection이 필요합니다.<br><br>**핵심:** Bottleneck 구조에서는 연산 비용을 줄이기 위해 중간 단계에서 의도적으로 채널 수를 크게 줄입니다. 하지만 이 상태로는 표현력이 부족해질 수 있기 때문에, 마지막 단계에서 채널을 다시 확장해 다음 블록에서 사용할 충분한 표현 공간을 복원합니다. 이로써 중간 단계는 계산 효율, 마지막 단계는 표현력 확보라는 역할로 분리되고, 전체 구조는 “저차원에서 계산하고 고차원으로 복원하는” 깊은 네트워크에 최적화된 효율 설계가 됩니다. |

---

## 4) Stage 구조와 Downsampling

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| Stage-based Design | Figure 3 / Table 1 | `layer1~layer4` | **비유:** “책을 챕터로 나눠, 한 챕터에서는 같은 글자 크기(해상도)로 읽다가 챕터가 바뀔 때만 레이아웃을 바꾸는 것.”<br><br>**수학적 근거:** stage 내부에서는 (H,W,채널 패턴)이 일정하고, stage 경계에서 stride/채널이 바뀝니다. 이는 네트워크가 스케일별 특징을 안정적으로 학습하게 합니다.<br><br>**핵심:** 네트워크가 깊어질수록 한 층에서 처리해야 할 정보의 성격은 점점 국소적인 디테일에서 추상적인 의미로 바뀝니다. Stage 구조는 해상도와 채널 수를 일정 구간마다만 바꾸도록 제한해, 각 구간이 하나의 스케일에 집중해서 특징을 학습하도록 설계된 구조입니다. 이로써 얕은 stage는 세밀한 공간 패턴을, 깊은 stage는 보다 추상적인 의미를 담당하게 되며, 멀티스케일 표현을 자연스럽게 쌓아 올리는 커리큘럼이 형성됩니다. |
| Downsampling at Stage Start | Table 1 | `stride=2` (첫 block) | **비유:** “지도에서 아무 데서나 줌 아웃하면 길을 잃으니, 챕터 시작점에서만 딱 한 번 줌 아웃.”<br><br>**수학적 근거:** 첫 블록 stride=2가 공간 해상도를 줄이고(요약), 보통 채널을 늘립니다(표현력 확보). 더하기를 유지하려면 shortcut도 같은 차원으로 맞춰야 합니다(Projection 또는 Option A).<br><br>**핵심:** 해상도를 줄이는 다운샘플링은 불가피하지만, 잘못 쓰면 중요한 공간 정보를 한꺼번에 잃을 위험이 있습니다. ResNet은 이 위험을 네트워크 전체에 흩뿌리지 않고, 스테이지가 시작되는 지점에만 다운샘플링을 집중시켜 정보 손실이 일어나는 위치를 명확하게 관리합니다. 또한 이 구간에서도 residual 경로를 유지해, 요약 과정에서 생기는 충격을 완화하고 다음 스테이지로의 정보 흐름을 안정적으로 이어가도록 설계했습니다. |

---

## 5) CIFAR-10 코드에서 다르게 구현한 부분

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| 6n+2 Depth Rule | 4.2. CIFAR-10 and Analysis | `depth = 6n+2` | **비유:** “레고가 ‘구역당 n세트’로 고정이라, n만 바꾸면 전체 높이가 정확히 예측되는 설명서.”<br><br>**수학적 근거:** (블록당 conv 2개)×(3스테이지×n블록)=6n, 여기에 stem conv 1개 + 마지막 분류층 1개를 더해 **6n+2**가 됩니다.<br><br>**핵심:** 깊이를 늘린다는 것은 여러 요소를 동시에 바꾸기 쉬운 작업입니다. 그래서 CIFAR-10 실험에서는 깊이 외의 조건을 모두 고정한 채, 블록 개수 n 하나만 조절해 네트워크를 확장하도록 설계했습니다. 이 규칙은 모델을 편하게 만들기 위한 장치가 아니라, 깊이 증가 그 자체가 학습과 성능에 어떤 영향을 미치는 지를 혼선 없이 관찰하기 위한 실험적 통제 구조입니다. |
| Identity Shortcut Only | 4.2. CIFAR-10 and Analysis | `shortcut="zero_pad"` | **비유**: “비교 실험인데 누군가만 어댑터(추가 부품)를 쓰면 반칙이니, 추가 부품 없이 규격만 맞추는 것.”<br><br>**수학적 근거**: projection(학습되는 W_s)을 쓰지 않고, 필요한 경우에만 zero-pad(고정)로 차원을 맞춰 F(x)+x를 유지합니다.<br><br>**핵심**: CIFAR-10 실험의 목표는 성능을 최대한 끌어올리는 것이 아니라, 잔차 연결 구조 자체가 학습을 쉽게 만드는지 확인하는 데 있었습니다. 이를 위해 projection처럼 추가 학습 파라미터가 들어가는 shortcut을 배제하고, 필요한 경우에만 zero-padding으로 차원만 맞춘 채 residual 구조를 유지했습니다. 이 설정에서 성능이 향상된다면, 그 원인은 모델 용량 증가가 아니라 잔차 경로가 만들어낸 최적화 구조 자체의 효과라고 보다 강하게 결론지을 수 있습니다. |
| Small Stem | 4.2. CIFAR-10 and Analysis | `3×3 conv` | **비유:** “32×32 미니 사진을 7×7 큰 도장으로 찍으면 디테일이 뭉개지니, 작은 도장(3×3)으로 조심히 시작.”<br><br>**수학적 근거:** CIFAR는 입력이 작아 큰 커널/초기 풀링이 정보를 과하게 잃습니다. 그래서 3×3 stem으로 공간 정보를 최대한 보존합니다.<br><br>**핵심:** 기존 모델에서 아무리 잔차학습으로 깊이를 확보해도, 초반 단계에서 공간적 디테일이 한 번에 사라지면 이후의 깊이는 그 정보를 되살릴 수 없습니다. 그래서 ResNet에서는 깊게 쌓는 혁신을 살리기 위해, 입력단부터 세밀한 정보를 최대한 보존하도록 입구(stem)를 더 섬세하게 설계합니다. |

---

## 6) Global Average Pooling

| 요소 | 논문 수식 / 그림 | 코드 좌표 | 핵심 설명 |
| --- | --- | --- | --- |
| Global Avg Pool | Table 1 | `AdaptiveAvgPool2d` | **비유**: 각 채널을 “특정 패턴을 감지하는 센서”라고 보면, Global Average Pooling은 그 센서가 이미지 전체에서 얼마나 자주, 강하게 반응했는지의 평균 점수만 남기는 방식이다.<br><br>**수학적 근거**: 각 채널의 feature map(H×W)을 전부 평균내어 채널당 하나의 값으로 만들기 때문에, 큰 Fully Connected 층 없이도 분류가 가능하고 파라미터 수가 크게 줄어든다.<br><br>**핵심**: 합성곱 층이 깊어질수록 특징의 위치 정보는 점점 덜 중요해지고, 그 특징이 존재하는지 여부 자체가 더 중요해집니다. Global Average Pooling은 각 채널의 반응을 공간 전체에서 평균내어, 특정 패턴이 어디에 나타났는지가 아니라 얼마나 일관되게 나타났는지를 기준으로 판단하도록 만듭니다. 이 과정은 파라미터를 줄이기 위한 단순한 압축이 아니라, 모델이 위치에 집착하지 않고 의미 자체에 집중하도록 유도하는 구조적 규제(regularization) 역할을 합니다. |
